{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import fileinput\n",
    "import os, glob\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Test is this out Check\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords   #This is just a test to see the features of liveshare\n",
    "\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract News Data from the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and extract the dataset\n",
    "\n",
    "The first step is to download and extract the dataset. The dataset can be downloaded from here, and then extracted by unzipping the file. After extracting the data, you should have a directory named 16119_db21c91a1ab47385bb13773ed8238c31. The directory contains 31 JSONL files, which you will iterate over to extract the text and title from each of the dictionaries. Finally, you will put these values in a list called dataset and target respectively. The length of the list dataset and target will be 94403. So essentially our dataset size is about 100K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94403\n",
      "94403\n"
     ]
    }
   ],
   "source": [
    "# Set path of folder with json files\n",
    "path = '/home/nelly88/Projects/Summarize COVID-19 News Using NLP and PyTorch/16119_db21c91a1ab47385bb13773ed8238c31/'\n",
    "\n",
    "# Create dataset list to store features and target list to store target values\n",
    "dataset = []\n",
    "target = []\n",
    "\n",
    "\n",
    "# Iterate through the first two json files in the folder\n",
    "for filename in sorted(os.listdir(path))[:2]: \n",
    "     if filename.endswith('.json'):\n",
    "         \n",
    "         # Open the files and read through each line\n",
    "         with open(os.path.join(path, filename), 'r') as f: # This returns f as a list of two strings/lines i.e. the full path of the first two json files\n",
    "             lines = f.readlines() # Reads throug each line of strings in the list and stores the contents in lines\n",
    "        # Iterate through each line, load the json files, extract the value of the text key and title and append to dataset and target respectively\n",
    "         for line in lines:\n",
    "             dataset.append(json.loads(line)['text']) \n",
    "             target.append(json.loads(line)['title'])\n",
    "\n",
    "print(len(dataset))\n",
    "print(len(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Clean Up\n",
    "\n",
    "Text cleanup is the process of cleaning up text data so that it is ready for use in a natural language processing application. The goal is to make the data as clean and consistent as possible so that the model can accurately learn from it. The steps include but not limited toe: converting text to lowercase, splitting sentences into individual words, applying the contraction hashmap on all the words of the text, removing the stopwords that are in English, removing apostrophes, using regular expression to remove parentheses outside a word, and using regular expression to remove punctuations. Finally, space characters are added before and after full stops. These steps are included in the `preprocess` function, which is then applied to the dataset and target. The length of most text is around 600, and the length of most target summary sentences are about 30. Any input beyond these lengths is disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a contraction hashmap\n",
    "contraction_map = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() # lowercase\n",
    "    text = text.split() # convert have'nt -> have not\n",
    "    # Apply the contraction hashmap on all the words of the text\n",
    "    for i in range(len(text)):\n",
    "        word = text[i]\n",
    "        if word in contraction_map:\n",
    "            text[i] = contraction_map[word]\n",
    "    text = \" \".join(text)\n",
    "    text = text.split()\n",
    "    newtext = []\n",
    "    # Remove the stopwords that are in English\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            newtext.append(word)\n",
    "    text = \" \".join(newtext)\n",
    "    # Remove 's. For example your's becomes your\n",
    "    text = text.replace(\"'s\",'') # convert your's -> your\n",
    "    # Remove parentheses outside a word. For example (word) becomes word\n",
    "    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n",
    "    # Remove punctuations\n",
    "    text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n",
    "    # Add a space character before and after the full stop. For example . becomes .\n",
    "    text = re.sub(r'\\.',' . ',text)\n",
    "    return (text)\n",
    "\n",
    "# Call the preprocess(text) function for all the items of dataset and target. Store the results in X and Y respectively.\n",
    "X = [preprocess(text) for text in dataset]\n",
    "Y = [preprocess(text) for text in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_text = 600 \n",
    "max_len_target = 30\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "# For each text in dataset\n",
    "for i in range(len(dataset)): \n",
    "    # If the length of the text is less than the max length\n",
    "    if(len(target[i].split())<=max_len_target and len(dataset[i].split())<=max_len_text):\n",
    "        # Append the text to the short_text list and the summary to the short_summary list\n",
    "        short_text.append(dataset[i])\n",
    "        short_summary.append(target[i])\n",
    "        \n",
    "# Create a dataframe with the short_text and short_summary\n",
    "temp_df=pd.DataFrame({'text':short_text,'summary':short_summary})\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty strings from summary and the text column\n",
    "newdf = temp_df[temp_df['summary'].str.strip().astype(bool)] \n",
    "df = newdf[newdf['text'].str.strip().astype(bool)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text feature generation\n",
    "\n",
    "After the text cleanup, we need to convert the text into numerical representations through feature generation. \n",
    "One way to generate features is to use a technique called one-hot vector. This involves converting each word into a vector with a single element. Because there are many words, the vector will be huge. However, we can use a trick to trim the data down to a few thousand words.\n",
    "\n",
    "\n",
    "\n",
    "We need to create a hashmap that keeps track of when each word first appeared in the text. We will also need a hashmap that maps word indices to words themselves, as well as a hashmap that counts the number of occurrences of each word. We will use these hashmaps later to replace rare words.\n",
    "\n",
    "Finally, we need to mark the start and end of each sentence in the target list. We can do this using special tokens called SOS_token and EOS_token.\n",
    "\n",
    "The `Lang` class keeps word -> index (word2index) and index -> word (index2word) dictionaries, as well as a count of each word word2count to use to later replace rare words.\n",
    "\n",
    "The `addSentence` method tokenizes the sentence into words using split, and adds each word to the dictionaries.\n",
    "\n",
    "The `addWord` method adds a word to both dictionaries, if it is not already there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "# Define a class Lang\n",
    "class Lang:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        # Create a hashmap word2index that keeps track of when each word first appeared in the text. This is for both the X and Y.\n",
    "        self.word2index = {}\n",
    "        # Create a hashmap index2word to keep track of which index is which word. \n",
    "        self.word2count = {}\n",
    "        # Create a separate hashmap word2count to count the number of occurrences of each word. We will need this later to replace rare words.\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"} \n",
    "        self.n_words = 2 # Count SOS and EOS\n",
    "\n",
    "    # Add a sentence to the hashmap word2index and word2count\n",
    "    def addSentence(self, sentence): \n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    # Add a word to the hashmap word2index and word2count \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the features ready for the model\n",
    "\n",
    "We need to apply the `Lang` class to the data and features to get them ready for the model. A function, `readData`, is defined to take text and summary as input. This function creates a tuple from the text and summary, and then creates an input and output object by passing the text and summary to the Lang class. The function then returns the input, output, and pairs. Another function, `prepareData`, is defined to take the list of df['text'] and list of df['summary'] as input. This function calls readData(X,Y) and gets back the input, output, and pairs. For each item in the pairs list, the input.addSentence(pair0) and output.addSentence(pair1) functions are called. Finally, the `prepareData` function returns the input, output, and pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 62358 sentence pairs\n",
      "Counting words...\n"
     ]
    }
   ],
   "source": [
    "# Create lists of text and summary strings from the dataframe\n",
    "text_X = list(df['text'])\n",
    "summary_Y = list(df['summary'])\n",
    "\n",
    "# Define a function readData that takes text and summary as input\n",
    "def readData(text, summary):\n",
    "    print(\"Reading lines...\")\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[text[i], summary[i]] for i in range(len(text))] \n",
    "    input_lang = Lang(text)\n",
    "    output_lang = Lang(summary)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# Define prepareData function that calls readData and returns the input, output, and pairs\n",
    "def prepareData(text, summary): \n",
    "    input_lang, output_lang, pairs = readData(text, summary) # Note that readData takes in the same parameters as prepareData\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        # Add the sentences to the input and output vocabularies\n",
    "        input_lang.addSentence(pair[0]) \n",
    "        output_lang.addSentence(pair[1])\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# Call the prepareData function and store the results in input, output, and pairs\n",
    "input_lang, output_lang, pairs = prepareData(text_X, summary_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random.choice(pairs)) # What does this do?\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an Attention Based Deep Learn. Model for Abstractive Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Sequence-to-Sequence Model\n",
    "\n",
    "The goal of this section is to build a sequence-to-sequence model with Attention for summarizing the news data. This is done by first converting the input sequence to a fixed-length vector using an encoder. The decoder then uses this vector to generate a shorter summary of the text. The attention network figures out which part of the sequence to pay attention to in the decoder network. This allows the model to handle long sequences of text effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the encoder class\n",
    "\n",
    "In the cell below,  we defined the EncoderRNN class. We also initialized the hidden_size and input_size parameters, and we defined the forward method that takes in an input and a hidden state and returns an output and a new hidden state. Finally, we also created a initHidden method that initializes the hidden state to zeros.\n",
    "\n",
    "\n",
    "\n",
    "The encoder class is a recurrent neural network that takes in input data and hidden layer initialization parameters. The input_size and hidden_size parameters are set when the encoder is initialized, as well as the embedding layer. The forward propagation of the neural network is executed using the reshaped embedding and the hidden layer input. The initHidden method initializes the initial hidden input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = max_len_text # Maximum number of words in a sequence\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the (Attention) decoder class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the class is doing:\n",
    "\n",
    "The initializations are pretty straightforward. We're deriving from nn.Module, so we have to call ```super().__init__()```\n",
    "to properly initialize it. We also define the various layers we'll be using.\n",
    "\n",
    "The forward function is where the magic happens. We take in an input word, the last hidden state, and all of the\n",
    "encoder's hidden states. The input word is passed through the embedding layer, then dropped out. Then, it's\n",
    "concatenated with the previous hidden state, and passed through a linear layer (called attn in the code). This\n",
    "layer will return an attention weight for each encoder hidden state.\n",
    "\n",
    "We then take the weighted sum of the encoder hidden states, using the attention weights as the weights. This is\n",
    "called the context vector, as it represents the \"context\" of the sentence we're translating.\n",
    "\n",
    "The context vector and the last hidden state are then passed through the second linear layer, which will make the\n",
    "final hidden state. This is used to get a score for each word in the output vocabulary.\n",
    "\n",
    "The output of the decoder is the result of passing the hidden state through the linear layer, then taking the\n",
    "log softmax of that result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the training data to tensors\n",
    "\n",
    "The next step is to convert the the list of pairs of sentences into tensors so that they can be used for training. torch.tensor is used for this purpose. This will help ensure that the training data is in a format that can be used by the model.\n",
    "\n",
    "The `indexesFromSentence` function takes a sentence and a Lang object as input. It then returns a list of indexes corresponding to the words in the sentence. The `tensorFromSentence` function takes a sentence and a Lang object as input. It then returns a tensor of indexes corresponding to the words in the sentence. The `tensorsFromPair` function takes a pair and two Lang objects as input. It then returns a tuple of tensors corresponding to the input and output sentences. The `tensorsFromPairs` function takes a list of pairs and two Lang objects as input. It then returns a list of tuples of tensors corresponding to the input and output sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')] \n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Sequence-to-Sequence Model\n",
    "\n",
    "The `train` method takes the input and output tensors, the encoder, decoder, encoder optimizer, decoder optimizer, criterion, and the maximum length of the output as input. It then loops through the input and output tensors, and for each pair, it initializes the encoder and decoder hidden states. It then loops through the input tensor and passes the input tensor and hidden state to the encoder. The encoder outputs the output and hidden state. The decoder input is initialized to SOS_token. The decoder hidden state is initialized to the encoder hidden state. The decoder then loops through the output tensor and passes the decoder input, decoder hidden state, and encoder output to the decoder. The decoder outputs the output, decoder hidden state, and decoder attention weights. The decoder input is set to the index of the word with the highest output value. The loss is calculated by comparing the decoder output to the target output. The encoder and decoder gradients are zeroed out. The encoder and decoder gradients are backpropagated. The encoder and decoder optimizers are stepped. The loss is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The train method\n",
    "\n",
    "The `train` method takes the input and output tensors, the encoder, decoder, encoder optimizer, decoder optimizer, criterion, and the maximum length of the output as input. The input_tensor and target_tensor are the Tensors that contain the input and target sentences. The encoder and decoder are the models. The encoder_optimizer and decoder_optimizer are the optimizers for their respective models. First, we set the encoder’s initial hidden state to be all zeros. Then we set up the optimizers, making sure to zero out their gradients first. Next, we get the length of the input and target sentences. We also create a Tensor of zeros that will hold the encoder’s output vectors. This will help us visualize what the attention of the decoder is doing while it is training. Then we loop through each time step in the input sentence. At each time step, we get an output vector (hidden state) from the encoder. We store the encoder’s output vectors. Then we set the initial input to the SOS token for the decoder. We also set the initial hidden state to be the encoder’s final hidden state. Then we decide if we are going to use teacher forcing or not. Teacher forcing is the technique where the target word is passed as the next input to the decoder. See the next section for more details about teacher forcing. If we are using teacher forcing, we use the target sentence as the next input to the decoder. If we are not using teacher forcing, we use the decoder’s prediction as the next input to the decoder. We then calculate the loss, and call loss.backward() to do backpropagation. Finally, we call the optimizer’s step function to update the parameters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    print(input_length)\n",
    "\n",
    "    for i in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "        #print(encoder_output.size())\n",
    "        encoder_outputs[i] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The trainIters method\n",
    "\n",
    "The `trainIters` method trains a sequence-to-sequence model using gradient descent. The number of iterations to train, the learning rate, and the optimizers for the encoder and decoder objects must be defined. The input and target tensors are converted to arrays and the loss is calculated. The \"train\" method is called num_iters times. Log messages can be added to track the start and end of training. The loss can be saved to monitor the model's performance over time.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, learning_rate=0.01):\n",
    "    print(\"Training....\")\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        if iter% 1000 == 0:\n",
    "            print(iter,\"/\",n_iters + 1)\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "The `infer` functions takes four arguments: the encoder, decoder, sentence, and max_length. The max_length argument sets the maximum number of words in the output sentence. For every pair in the input data, the infer method outputs two values: output words and attentions. The output sentence is simply a string consisting of the output words. The evaluation_input.txt file contains a list of target summary sentences and their corresponding output sentences. This file will be used in model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        output_words, attentions = infer(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        with open('evaluation_input.txt','a') as out:\n",
    "            out.write('{}, {}\\n'.format(pair[1],output_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    " First, create an encoder object from the Encoder class and a decoder object from the AttnDecoder class. Then, pass those two objects along with 1000 as the parameter to the `trainIters` function and see the training happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder, attn_decoder, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "\n",
    "The model can be saved as a state_dict, which is a Python dictionary that stores the current state of the model. The model can also be saved as a tar file, which stores the entire model including the model parameters, the optimizer parameters, and the state_dict. The model can be loaded using the `load_state_dict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as state_dict for Inference\n",
    "\n",
    "ENCODER_MODEL_PATH_10 = 'encoder_10.pth'\n",
    "DECODER_MODEL_PATH_10 = 'decoder_10.pth'\n",
    "torch.save(encoder.state_dict(), ENCODER_MODEL_PATH_10)\n",
    "torch.save(attn_decoder.state_dict(), DECODER_MODEL_PATH_10)\n",
    "\n",
    "# Save the model as state_dict for Inference\n",
    "ENCODER_ENITRE = 'encoder_entire.pth'\n",
    "DECODER_ENITRE = 'decoder_entire.pth'\n",
    "torch.save(encoder, ENCODER_ENITRE)\n",
    "torch.save(attn_decoder, DECODER_ENITRE)\n",
    "\n",
    "\n",
    "ENCODER_PARAMS = 'encoder_params.tar'\n",
    "learning_rate=0.01\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "torch.save({\n",
    "            'epoch': 10,\n",
    "            'model_state_dict': encoder.state_dict(),\n",
    "            'optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, ENCODER_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder, attn_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate summarization results with ROUGE score\n",
    "\n",
    "We will use Rouge score to evaluate the summarization results of the models. ROUGE assigns a score to your model based on the similarity of words between a human-generated summary and a machine-generated summary. That means, ROUGE simply counts the number of n-grams that are similar in your model-generated summary with respect to a human-generated summary. For example, ROUGE-1 means the overlap of unigrams between the model-generated summary and the human-generated summary. ROUGE-2 means the overlap of bigrams and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each line from input text and create a list of tuple\n",
    "def read_input(filename = 'evaluation_input.txt'):\n",
    "    input_pair = []\n",
    "    with open(filename) as fp:\n",
    "        for cnt, line in enumerate(fp):\n",
    "            temp = []\n",
    "            #print(line)\n",
    "            res = line.split(',')\n",
    "            #print(res[0])\n",
    "            #print(res[1])\n",
    "            temp.append(res[0].rstrip('\\n'))\n",
    "            temp.append(res[1].rstrip('\\n'))\n",
    "            input_pair.append(temp)\n",
    "    return input_pair\n",
    "\n",
    "# Write scores to a file\n",
    "def write_score(scores):\n",
    "    # write scores to a file. This file is the out of this milestone\n",
    "     with open('score.txt','a') as out:\n",
    "        out.write(json.dumps(scores))\n",
    "     \n",
    "    \n",
    "# Define a scoring function to instantiate the rogue_scorer object\n",
    "def scoring(input_pair):\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "    for pair in input_pair:\n",
    "        scores = scorer.score(pair[0],pair[1])\n",
    "        print(scores)\n",
    "        write_score(scores)\n",
    "\n",
    "\n",
    "input_pair = read_input()\n",
    "scoring(input_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Your Models with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Tensorboard\n",
    "\n",
    "Setting up Tensorboard is simple – all you need is the TensorFlow installation and the pip install of Tensorboard. Once you have these, you can create a SummaryWriter object that will store the data to be visualized by Tensorboard. Make sure to direct this output to a directory that Tensorboard can access. You can start the Tensorboard service by typing 'tensorboard --logdir=runs' in your terminal, which will give you a message telling you the URL to access Tensorboard. In your browser, go to this URL and you will see the Tensorboard interface. Right now, there is no data to visualize because we have not written anything to Tensorboard yet. We will do this next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SummaryWriter from torch.utils.tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Create a SummaryWriter object writer by specifying a directory from which the Tensorboard will get the data to visualize\n",
    "writer = SummaryWriter('runs/summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with TensorBoard\n",
    "\n",
    "You can use TensorBoard to observe training loss of each iteration, total training loss over each epoch, encoder parameters and decoder parameters. To do this, you first need to define a function to write the data to TensorBoard. You can then call this function in the code for your program. Finally, you can use the TensorBoard tool to view the data.\n",
    "\n",
    "TensorBoard takes different types of input and creates different representations of them, such as scalars, image, text, histogram, graph, embedding, and so on. In this project, we focused on scalars and histograms. Scalars are for the metrics we generally think of such as loss, accuracy, and so on. which we can visualize using `.add_scalars`. We can also visualize different model parameters with `.add_histogram`. For example, we can observe training loss of each iteration, total training loss over each epoch, encoder parameters and decoder parameters. TensorBoard is a useful tool for visualizing training data and observing model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 1000\n",
    "\n",
    "# For tensorboard visualization       \n",
    "def variable2numpy(var):  \n",
    "    return var.data.cpu().numpy()\n",
    "\n",
    "# Define a function write_to_tensorboard(writer, loss, total_loss, encoder, decoder). Here writer is the SummaryWriter object\n",
    "def write_to_tensorboard(writer, loss, total_loss, encoder, decoder):\n",
    "    \n",
    "    # Write the scalars to tensorboard as\n",
    "    writer.add_scalars('loss in each iteration', {'loss':loss})\n",
    "    writer.add_scalars('total loss', {'total loss':total_loss})\n",
    "    \n",
    "    # Write the encoder parameter using .add_histogram method\n",
    "    for name, param in encoder.named_parameters():\n",
    "        name = name.replace('.', '/')\n",
    "        writer.add_histogram('encoder/{}'.format(name), variable2numpy(param), global_step, bins='auto')\n",
    "        if param.grad is not None:\n",
    "            writer.add_histogram('encoder/{}/grad'.format(name), variable2numpy(param.grad), global_step, bins='auto')\n",
    "            \n",
    "    # Write the decoder parameter using .add_histogram method\n",
    "    for name, param in decoder.named_parameters():\n",
    "        name = name.replace('.', '/')\n",
    "        writer.add_histogram('decoder/{}'.format(name), variable2numpy(param), global_step, bins='auto')\n",
    "        if param.grad is not None:\n",
    "            writer.add_histogram('decoder/{}/grad'.format(name), variable2numpy(param.grad), global_step, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        write_to_tensorboard(writer, loss, plot_loss_total, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SummaryWriter from torch.utils.tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Create a SummaryWriter object writer by specifying a directory from which the Tensorboard will get the data to visualize\n",
    "writer = SummaryWriter('runs/summary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate on Hyperparameters to Improve your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many hyperparameters can impact the performance of a deep learning model, but not all have the same impact. For this project, we will focus on two - the learning rate and the size of hidden layers. We will experiment with different values for each to see how they affect the model's accuracy and efficiency. By training the model with different values for these parameters, we can find the optimal values that produce the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('myvenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e0c61386e250da26998f0368a5cee9f99ef9564ea99b9d45d43fe7f9e8d227b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
